{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 1. Load tokenizer dan model IndoBERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
    "model = BertModel.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
    "\n",
    "# 2. Contoh input teks\n",
    "input_text = \"Ini adalah contoh teks.\"\n",
    "tokens = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# 3. Mendapatkan embeddings dari token input\n",
    "input_embeddings = model.embeddings(tokens['input_ids'])\n",
    "\n",
    "# 4. Mengakses bobot Q, K, V pada layer pertama dan mencetak bobot awal sebelum update\n",
    "layer_index = 0\n",
    "query_weights = model.encoder.layer[layer_index].attention.self.query.weight\n",
    "key_weights = model.encoder.layer[layer_index].attention.self.key.weight\n",
    "value_weights = model.encoder.layer[layer_index].attention.self.value.weight\n",
    "\n",
    "print(\"Bobot awal Query (sebelum update):\", query_weights)\n",
    "print(\"Bobot awal Key (sebelum update):\", key_weights)\n",
    "print(\"Bobot awal Value (sebelum update):\", value_weights)\n",
    "\n",
    "# 5. Menghitung nilai Q, K, V untuk input embeddings\n",
    "query = torch.matmul(input_embeddings, query_weights.T)\n",
    "key = torch.matmul(input_embeddings, key_weights.T)\n",
    "value = torch.matmul(input_embeddings, value_weights.T)\n",
    "\n",
    "# 6. Perhitungan attention scores\n",
    "d_k = query.size(-1)  # Dimensi Q atau K\n",
    "attention_scores = torch.matmul(query, key.transpose(-1, -2)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "attention_probs = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "# 7. Menghasilkan attention output\n",
    "attention_output = torch.matmul(attention_probs, value)\n",
    "\n",
    "# 8. Feed-forward processing dalam layer encoder (berulang sesuai layer model IndoBERT)\n",
    "layer_output = model.encoder.layer[layer_index].output.dense(attention_output)\n",
    "\n",
    "# 9. Mengambil representasi dari token [CLS] untuk klasifikasi\n",
    "cls_representation = model.pooler.dense(layer_output[:, 0, :])  # Representasi klasifikasi\n",
    "\n",
    "# 10. Layer klasifikasi sederhana dengan 2 kelas\n",
    "classification_layer = nn.Linear(model.config.hidden_size, 2)\n",
    "logits = classification_layer(cls_representation)\n",
    "predictions = torch.argmax(logits, dim=-1)  # Mengambil prediksi kelas\n",
    "\n",
    "# 11. Definisikan label target untuk contoh ini\n",
    "labels = torch.tensor([1])  # Misalnya label kelas positif\n",
    "\n",
    "# 12. Hitung loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(logits, labels)\n",
    "\n",
    "# 13. Backward pass untuk menghitung gradien\n",
    "loss.backward()\n",
    "\n",
    "# 14. Optimizer (misalnya Adam)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Melihat bobot sebelum update untuk satu layer\n",
    "for name, param in model.named_parameters():\n",
    "    if \"encoder.layer.0.attention.self.query.weight\" in name:\n",
    "        print(\"Bobot Query sebelum update:\", param.data)\n",
    "\n",
    "# 15. Update bobot\n",
    "optimizer.step()\n",
    "\n",
    "# 16. Melihat bobot setelah update untuk layer yang sama\n",
    "for name, param in model.named_parameters():\n",
    "    if \"encoder.layer.0.attention.self.query.weight\" in name:\n",
    "        print(\"Bobot Query setelah update:\", param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 1. Load tokenizer dan model IndoBERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
    "model = BertModel.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
    "\n",
    "# 2. Contoh input teks\n",
    "input_text = \"Ini adalah contoh teks.\"\n",
    "tokens = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# 3. Mendapatkan embeddings dari token input\n",
    "input_embeddings = model.embeddings(tokens['input_ids'])\n",
    "\n",
    "# 4. Mengakses bobot Q, K, V pada layer pertama dan mencetak bobot awal sebelum update\n",
    "layer_index = 0\n",
    "query_weights = model.encoder.layer[layer_index].attention.self.query.weight\n",
    "key_weights = model.encoder.layer[layer_index].attention.self.key.weight\n",
    "value_weights = model.encoder.layer[layer_index].attention.self.value.weight\n",
    "\n",
    "print(\"Bobot awal Query (sebelum update):\", query_weights)\n",
    "print(\"Bobot awal Key (sebelum update):\", key_weights)\n",
    "print(\"Bobot awal Value (sebelum update):\", value_weights)\n",
    "\n",
    "# 5. Menghitung nilai Q, K, V untuk setiap kata dalam input embeddings\n",
    "query = torch.matmul(input_embeddings, query_weights.T)\n",
    "key = torch.matmul(input_embeddings, key_weights.T)\n",
    "value = torch.matmul(input_embeddings, value_weights.T)\n",
    "\n",
    "# 6. Menghitung skor perhatian antara setiap pasangan kata\n",
    "d_k = query.size(-1)\n",
    "attention_scores = torch.matmul(query, key.transpose(-1, -2)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "\n",
    "# 7. Normalisasi softmax pada skor perhatian untuk mendapatkan attention weights\n",
    "attention_probs = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "# 8. Hitung representasi konteksual untuk setiap kata berdasarkan attention weights\n",
    "contextual_representation = torch.matmul(attention_probs, value)\n",
    "\n",
    "# 9. Feed-forward processing dalam layer encoder (berulang sesuai layer model IndoBERT)\n",
    "layer_output = model.encoder.layer[layer_index].output.dense(contextual_representation)\n",
    "\n",
    "# 10. Mengambil representasi dari token [CLS] untuk klasifikasi\n",
    "cls_representation = model.pooler.dense(layer_output[:, 0, :])\n",
    "\n",
    "# 11. Layer klasifikasi sederhana dengan 2 kelas\n",
    "classification_layer = nn.Linear(model.config.hidden_size, 2)\n",
    "logits = classification_layer(cls_representation)\n",
    "predictions = torch.argmax(logits, dim=-1)  # Mengambil prediksi kelas\n",
    "\n",
    "# 12. Definisikan label target untuk contoh ini\n",
    "labels = torch.tensor([1])  # Misalnya label kelas positif\n",
    "\n",
    "# 13. Hitung loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(logits, labels)\n",
    "\n",
    "# 14. Backward pass untuk menghitung gradien\n",
    "loss.backward()\n",
    "\n",
    "# 15. Optimizer (misalnya Adam)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# 16. Logging bobot Q, K, V sebelum update\n",
    "for name, param in model.named_parameters():\n",
    "    if \"encoder.layer.0.attention.self.query.weight\" in name:\n",
    "        print(\"Bobot Query sebelum update:\", param.data)\n",
    "    if \"encoder.layer.0.attention.self.key.weight\" in name:\n",
    "        print(\"Bobot Key sebelum update:\", param.data)\n",
    "    if \"encoder.layer.0.attention.self.value.weight\" in name:\n",
    "        print(\"Bobot Value sebelum update:\", param.data)\n",
    "\n",
    "# 17. Update bobot\n",
    "optimizer.step()\n",
    "\n",
    "# 18. Logging bobot Q, K, V setelah update\n",
    "for name, param in model.named_parameters():\n",
    "    if \"encoder.layer.0.attention.self.query.weight\" in name:\n",
    "        print(\"Bobot Query setelah update:\", param.data)\n",
    "    if \"encoder.layer.0.attention.self.key.weight\" in name:\n",
    "        print(\"Bobot Key setelah update:\", param.data)\n",
    "    if \"encoder.layer.0.attention.self.value.weight\" in name:\n",
    "        print(\"Bobot Value setelah update:\", param.data)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
